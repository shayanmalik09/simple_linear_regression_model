{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Comparing Gradient Descent Variants for Linear Regression**"
      ],
      "metadata": {
        "id": "Xu5dZsP5IFsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Gradient Descent with Momentum\n",
        "def train_gd(X, y, epochs=500, learning_rate=0.01, batch_size=None, momentum=0.0):\n",
        "    num_features = X.shape[1]\n",
        "    weights = np.random.randn(num_features, 1)  # Randomly initialize weights\n",
        "    bias = np.random.randn(1)  # Randomly initialize bias\n",
        "    velocity_w = np.zeros((num_features, 1))  # Keeps track of past weight updates\n",
        "    velocity_b = 0  # Keeps track of past  updates\n",
        "    loss_history = []  # Store loss values over epochs\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        # If batch_size is set and smaller than the dataset then we pick random samples\n",
        "        if batch_size and batch_size < len(y):\n",
        "            indices = np.random.choice(len(y), batch_size, replace=False)\n",
        "            X_batch = X[indices]\n",
        "            y_batch = y[indices].reshape(-1, 1)\n",
        "        else:\n",
        "            X_batch = X\n",
        "            y_batch = y.reshape(-1, 1)\n",
        "\n",
        "        # Predict using current weights and bias\n",
        "        predictions = np.dot(X_batch, weights) + bias\n",
        "        errors = y_batch - predictions  # solve errors\n",
        "\n",
        "        # solve gradients\n",
        "        gradient_weights = -2 * np.dot(X_batch.T, errors) / len(X_batch)\n",
        "        gradient_bias = -2 * np.mean(errors)\n",
        "\n",
        "        # Apply momentum to smooth updates\n",
        "        velocity_w = momentum * velocity_w + (1 - momentum) * gradient_weights\n",
        "        velocity_b = momentum * velocity_b + (1 - momentum) * gradient_bias\n",
        "\n",
        "        # Update weights n bias\n",
        "        weights -= learning_rate * velocity_w\n",
        "        bias -= learning_rate * velocity_b\n",
        "\n",
        "        # solve loss i.e Mean Squared Error and store it\n",
        "        loss = np.mean(errors ** 2)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "    return weights, bias, loss_history\n",
        "\n",
        "# Generate synthetic dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(2500, 2)\n",
        "y = 3 * X[:, 0] + 5 * X[:, 1] + np.random.randn(2500) * 0.5  # Linear relation with some noise\n",
        "\n",
        "# Train models with different batch sizes\n",
        "w_batch, b_batch, loss_batch = train_gd(X, y, batch_size=None)  # Full-batch GD\n",
        "w_sgd, b_sgd, loss_sgd = train_gd(X, y, batch_size=1)  # Stochastic GD\n",
        "w_mini, b_mini, loss_mini = train_gd(X, y, batch_size=100)  # Mini-batch GD\n",
        "\n",
        "# Train models with different learning rates\n",
        "lr_values = [0.01, 0.05, 0.1]\n",
        "losses_lr = {}\n",
        "\n",
        "for lr in lr_values:\n",
        "    _, _, losses_lr[lr] = train_gd(X, y, learning_rate=lr, batch_size=100)\n",
        "\n",
        "# Train models with different momentum values\n",
        "momentum_values = [0.0, 0.9]\n",
        "losses_momentum = {}\n",
        "\n",
        "for m in momentum_values:\n",
        "    _, _, losses_momentum[m] = train_gd(X, y, learning_rate=0.05, batch_size=100, momentum=m)\n",
        "\n",
        "# Plot for Loss comparison for Batch GD, SGD, and Mini-batch GD\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_batch, label=\"Batch GD\")\n",
        "plt.plot(loss_sgd, label=\"SGD\")\n",
        "plt.plot(loss_mini, label=\"Mini-batch GD\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss Comparison: Batch vs. SGD vs. Mini-batch\")\n",
        "plt.show()\n",
        "\n",
        "# Plot for Effect of Learning Rate on Convergence\n",
        "plt.figure(figsize=(10, 5))\n",
        "for lr, loss in losses_lr.items():\n",
        "    plt.plot(loss, label=f\"Learning Rate {lr}\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Effect of Learning Rate on Convergence\")\n",
        "plt.show()\n",
        "\n",
        "# Plot for Effect of Momentum on Convergence\n",
        "plt.figure(figsize=(10, 5))\n",
        "for m, loss in losses_momentum.items():\n",
        "    plt.plot(loss, label=f\"Momentum {m}\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Effect of Momentum on Convergence\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vWzGxNBpMdvo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}